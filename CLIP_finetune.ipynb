{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748e27cf-e188-4225-9c8f-6e618ae5b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"amazon_com_ecommerce.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ebdc96c-4cde-4df2-9acf-546fe4b707e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10002 entries, 0 to 10001\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Uniq Id                10002 non-null  object\n",
      " 1   Product Name           10002 non-null  object\n",
      " 2   Category               9172 non-null   object\n",
      " 3   Selling Price          9895 non-null   object\n",
      " 4   Model Number           8230 non-null   object\n",
      " 5   About Product          9729 non-null   object\n",
      " 6   Product Specification  8370 non-null   object\n",
      " 7   Technical Details      9212 non-null   object\n",
      " 8   Shipping Weight        8864 non-null   object\n",
      " 9   Product Dimensions     479 non-null    object\n",
      " 10  Image                  10002 non-null  object\n",
      " 11  Variants               2478 non-null   object\n",
      " 12  Product Url            10002 non-null  object\n",
      " 13  Is Amazon Seller       10002 non-null  object\n",
      "dtypes: object(14)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna(axis=1,how=\"all\")\n",
    "df=df.drop(columns=[\"Upc Ean Code\",]) # delete additonal unuseful\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "026394df-88cc-4eee-8529-ed3c1ee1d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10002 entries, 0 to 10001\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Uniq Id                10002 non-null  object\n",
      " 1   Product Name           10002 non-null  object\n",
      " 2   Category               9172 non-null   object\n",
      " 3   Selling Price          9895 non-null   object\n",
      " 4   Model Number           8230 non-null   object\n",
      " 5   About Product          9729 non-null   object\n",
      " 6   Product Specification  8370 non-null   object\n",
      " 7   Technical Details      9212 non-null   object\n",
      " 8   Shipping Weight        8864 non-null   object\n",
      " 9   Product Dimensions     479 non-null    object\n",
      " 10  Image                  10002 non-null  object\n",
      " 11  Variants               2478 non-null   object\n",
      " 12  Product Url            10002 non-null  object\n",
      " 13  Is Amazon Seller       10002 non-null  object\n",
      " 14  text                   10002 non-null  object\n",
      "dtypes: object(15)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "def combine_text(row):\n",
    "    text_fields = [\n",
    "        row.get(\"Product Name\", \"\"),\n",
    "        row.get(\"About Product\", \"\")\n",
    "    ]\n",
    "    return \" \".join([str(f) for f in text_fields if pd.notna(f) and f.strip()])\n",
    "\n",
    "df[\"text\"] = df.apply(combine_text, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f28083c-4362-457a-bab3-c2d759c36522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10002/10002 [1:16:27<00:00,  2.18it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True) # create images folder\n",
    "\n",
    "valid_entries = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    url = row[\"Image\"]\n",
    "    text = row[\"text\"]\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        filename = f\"images/{row['Uniq Id']}.jpg\"\n",
    "        image.save(filename)\n",
    "        valid_entries.append({\"image_path\": filename, \"text\": text})\n",
    "    except Exception as e:\n",
    "        continue  # skip broken URLs\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame(valid_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e38f2-9272-444b-9c4e-71b8d1e10872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d910cc27-9aab-4f54-a30c-d21166f3fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data to local\n",
    "train_df.to_csv(\"clip_train_data.csv\", index=False)\n",
    "\n",
    "# load back train data\n",
    "train_df=pd.read_csv(\"clip_train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495c1ac-0f9c-4a1c-a587-2f3b847866ee",
   "metadata": {},
   "source": [
    "# finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14033772-bb97-4d0e-bac3-223fe971389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "# Define PyTorch Dataset\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor):\n",
    "        self.data = dataframe\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx][\"image_path\"]\n",
    "        text = self.data.iloc[idx][\"text\"]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Ensure text is truncated to 77 tokens\n",
    "        return self.processor(\n",
    "            text=[text],\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "        )\n",
    "\n",
    "# Load Pretrained CLIP Model\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fffd0e0b-c969-488f-bbc3-750f2b04b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Loss: 0.1067\n",
      "Epoch 2 — Loss: 0.0423\n",
      "Epoch 3 — Loss: 0.0361\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tuning Loop\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset = ProductDataset(train_df, processor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3):  # adjust epochs as needed\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        # Remove batch dimension\n",
    "        input_ids = batch[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].squeeze(1).to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].squeeze(1).to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            return_loss=True  # This enables contrastive loss automatically\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} — Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b0d2fbd-5a67-4deb-ae4d-34e9cbc6cac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save CLIP\n",
    "model.save_pretrained(\"clip-finetuned\")\n",
    "processor.save_pretrained(\"clip-finetuned\")\n",
    "\n",
    "# load back\n",
    "# from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\"clip-finetuned\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"clip-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd111704-1b8f-46d1-8337-690545410b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0290856-e1d3-40e2-96d9-27348c71cf29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
